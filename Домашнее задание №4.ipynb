{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c7419d8",
   "metadata": {},
   "source": [
    "# Домашнее задание № 4. Языковые модели\n",
    "\n",
    "## Задание 1 (8 баллов).\n",
    "\n",
    "В семинаре для генерации мы использовали предположение маркова и считали, что слово зависит только от 1 предыдущего слова. Но ничто нам не мешает попробовать увеличить размер окна и учитывать два или даже три прошлых слова. Для них мы еще сможем собрать достаточно статистик и, логично предположить, что качество сгенерированного текста должно вырасти.\n",
    "\n",
    "Попробуйте сделать языковую модель, которая будет учитывать два предыдущих слова при генерации текста. Сгенерируйте несколько текстов (3-5) и расчитайте перплексию получившейся модели. Можно использовать данные из семинара или любые другие (сопоставимые или большие по объему). Перплексию рассчитывайте на 10-50 отложенных предложениях (они не должны использоваться при сборе статистик).\n",
    "\n",
    "Подсказки:\n",
    "\n",
    "- нужно будет добавить еще один тэг <start>  \n",
    "- еще одна матрица не нужна, можно по строкам хронить биграмы, а по колонкам униграммы  \n",
    "- тексты должны быть очень похожи на нормальные (если у вас получается рандомная каша, вы что-то делаете не так). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afc84732",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from razdel import sentenize\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "from collections import Counter\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from scipy.sparse import csr_matrix, csc_matrix, lil_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab6c942b",
   "metadata": {},
   "outputs": [],
   "source": [
    "news = open('lenta.txt', encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8100a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    normalized_text = [word.text.strip(punctuation) for word \\\n",
    "                                                            in razdel_tokenize(text)]\n",
    "    normalized_text = [word.lower() for word in normalized_text if word and len(word) < 20 ]\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19935c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_news = normalize(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f98fb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_news = Counter(norm_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f8a266e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrammer(tokens, n=2):\n",
    "    ngrams = []\n",
    "    for i in range(0,len(tokens)-n+1):\n",
    "        ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d954e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_news = [['<start>'] + ['<start>'] + normalize(text) + ['<end>'] for text in sent_tokenize(news)][:-100]\n",
    "sentences_test = [['<start>'] + ['<start>'] + normalize(text) + ['<end>'] for text in sent_tokenize(news)][-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eebbe847",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams_news = Counter()\n",
    "bigrams_news = Counter()\n",
    "threegrams_news = Counter()\n",
    "\n",
    "for sentence in sentences_news:\n",
    "    unigrams_news.update(sentence)\n",
    "    bigrams_news.update(ngrammer(sentence))\n",
    "    threegrams_news.update(ngrammer(sentence, n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e8c06ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_news = lil_matrix((len(bigrams_news), \n",
    "                   len(unigrams_news)))\n",
    "\n",
    "id2word_unigrams_news = list(unigrams_news)\n",
    "word2id_unigrams_news = {word:i for i, word in enumerate(id2word_unigrams_news)}\n",
    "\n",
    "id2word_bigrams_news = list(bigrams_news)\n",
    "word2id_bigrams_news = {word:i for i, word in enumerate(id2word_bigrams_news)}\n",
    "\n",
    "for ngram in threegrams_news:\n",
    "    wordl, word2, word3 = ngram.split() \n",
    "    bigram = wordl + ' ' + word2\n",
    "    matrix_news[word2id_bigrams_news[bigram], word2id_unigrams_news[word3]] = (threegrams_news[ngram]/bigrams_news[bigram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d9afff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_news = csr_matrix(matrix_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ee255d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(matrix, id2word_bigrams_news, id2word_unigrams_news, word2id_bigrams_news, n=200, start='<start> <start>'):\n",
    "    text = []\n",
    "    current_idx = word2id_bigrams_news[start]\n",
    "    for i in range(n):\n",
    "        chosen = np.random.choice(list(range(matrix.shape[1])), p=matrix[current_idx].toarray()[0])\n",
    "        text.append(id2word_unigrams_news[chosen])\n",
    "        if id2word_unigrams_news[chosen] == '<end>':\n",
    "            current_idx = word2id_bigrams_news[start]\n",
    "        else:\n",
    "            part = id2word_bigrams_news[current_idx] + ' ' + id2word_unigrams_news[chosen]\n",
    "            part = ' '.join(part.split()[1:])\n",
    "            current_idx = word2id_bigrams_news[part]\n",
    "    \n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b2adc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = generate(matrix_news, id2word_bigrams_news, id2word_unigrams_news, word2id_bigrams_news).replace('<end>', '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "987a6158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "не совсем понимаю зачем распространяется подобная информация \n",
      "\n",
      " замгенпрокурора россии василий средин это стало неожиданностью для американских синоптиков \n",
      "\n",
      " как никто из моряков не могли найти англичане при обыске было обнаружено захоронение принадлежит лишенному сана католическому священнику доменику катарибао одному из членов российской делегации глава цик отметил что такая ситуация затрудняет взаимодействие сторон \n",
      "\n",
      " оформлением храма занимались творческие коллективы \n",
      "\n",
      " произошел перегрев подшипников и пошел дым \n",
      "\n",
      " они обсудили свое дальнейшее сотрудничество в расследовании дел связанных с реализацией положений закона о налоге на доходы физических лиц \n",
      "\n",
      " четыре члена экипажа и пассажиров выбросило наружу \n",
      "\n",
      " в других развитых странах отметил вешняков \n",
      "\n",
      " к первым безусловно можно отнести слабые структурную и налоговую политику \n",
      "\n",
      " напомним что один из двух партий вольфгангом шюсселем и йоргом хайдером подчеркивается что современное законодательство обязывает полицию безопасности обнародовать данные о пользователе позволяет подставлять вместо реального адреса e-mail произвольные липовые адреса и номера кредитных карт \n",
      "\n",
      " согласно французским законам в стране пресс-релизе мида находится в думе о возможности такой искалки будут наверняка ужесточены \n",
      "\n",
      " в результате своей беспечности \n",
      "\n",
      " у нас нет к международным санкциям \n",
      "\n",
      " автомобиль папе вручили председатель совета федерации не будет \n",
      "\n",
      " среди возможных вариантов фигурирует разделение корпорации совершенно неоправданный шаг \n",
      "\n",
      " как сообщили\n"
     ]
    }
   ],
   "source": [
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29ca3316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(probas):\n",
    "    p = np.exp(np.sum(probas))\n",
    "    N = len(probas)\n",
    "    return p ** (-1/N) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65bee573",
   "metadata": {},
   "outputs": [],
   "source": [
    "_perplexity = []\n",
    "for sent in sentences_test:\n",
    "    probs = []\n",
    "    for ngram in ngrammer(sent, 3):\n",
    "        word1, word2, word3 = ngram.split()\n",
    "        bigram = word1 + ' '+word2\n",
    "        \n",
    "        if ngram in threegrams_news and bigram in bigrams_news:\n",
    "            probs.append(np.log(threegrams_news[ngram]/bigrams_news[bigram]))\n",
    "        else:\n",
    "            probs.append(np.log(0.00001))\n",
    "    if perplexity(probs)!= np.inf: \n",
    "        _perplexity.append(perplexity(probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0cbc04ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20089.202373196247"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(_perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bc5741",
   "metadata": {},
   "source": [
    "## Задание № 2* (2 балла).\n",
    "\n",
    "1. Что можно делать с проблемой несловарных слов? В семинаре мы просто использовали какое-то маленькое значение вероятности, а какие есть другие способы?\n",
    "\n",
    "__Ответ: Можно создать фиксироанный словарь, а все слова, которые не входят в него, пометить тэгом \"UNK\" на этапе нормализации. Затем, для этих слов рассчитать вероятность как для любых других. Либо можно неявно создать словарь, а малочастнотные слова заменить на \"UNK\".__\n",
    "\n",
    "2. Что такое сглаживание (smoothing)?\n",
    "\n",
    "__Ответ: Сглаживание - это перераспределение вероятностей, то есть понижение вероятности для частых случаев в пользу случаев, которые мы раньше не видели или в которых не уверены (неизвестный контекст).__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
